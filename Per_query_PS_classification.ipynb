{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Per query PS classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okliviaf/PredictingPS/blob/master/Per_query_PS_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjzC3OSp6jyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install -U -q seaborn\n",
        "!pip install -U -q tensorflow-gpu==1.15.2\n",
        "from tensorflow import set_random_seed\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "from sklearn import svm\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn import preprocessing\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import optimizers\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import preprocessing\n",
        "import numpy\n",
        "from collections import namedtuple\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "Dataset = namedtuple(\"Dataset\", [\"X\", \"y\", \"num_features\", \"num_classes\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ZXQIqpeW1s",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader\n",
        "\n",
        "The data for the machine learning models training have been uploaded on Google Drive and made freely available. We use the Google Drive Python API in order to automatically download the data in Google Colab. \n",
        "\n",
        "Alternatively, if you decide to download the notebook and run it locally, please remember to set the variable `LOCAL_RUN` to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yngTG5165Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOCAL_RUN = False\n",
        "\n",
        "if not LOCAL_RUN:\n",
        "    # we create the 'data' folder that will contain the training data\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "    downloaded = drive.CreateFile({\"id\": \"1kyHohSD4iAYJWv6ryUQUpkPjbw1nOryO\"})\n",
        "    downloaded.GetContentFile(\"data/FA_Binary.csv\")\n",
        "    \n",
        "    downloaded = drive.CreateFile({\"id\": \"1zXWz49xBUU7iQ9AAokXWr_9vku3r1Yms\"})\n",
        "    downloaded.GetContentFile(\"data/NC_Binary.csv\")\n",
        "\n",
        "    downloaded = drive.CreateFile({\"id\": \"1qdeITwpqP2Xvwuu5j7BHbWriL1DW8XSM\"})\n",
        "    downloaded.GetContentFile(\"data/Overall_Trio.csv\")\n",
        "    \n",
        "FA_Binary = pd.read_csv(\"data/FA_Binary.csv\")\n",
        "NC_Binary = pd.read_csv(\"data/NC_Binary.csv\")\n",
        "Overall_Trio = pd.read_csv(\"data/Overall_Trio.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqSZIEJBgHyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"'Finding As' dataset size: {}\".format(FA_Binary.shape))\n",
        "print(\"'Number comparison' dataset size: {}\".format(NC_Binary.shape))\n",
        "print(\"'Overall' dataset size: {}\".format(Overall_Trio.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-GktO2ICWz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attributes = [\n",
        "  'no_of_actions',\n",
        "  'time_query',\n",
        "  'time_on_serp',\n",
        "  'time_on_documents',\n",
        "   'time_session_overall',\n",
        "    'serp_page_viewed_to',\n",
        "    'document_click_count',\n",
        "    'document_click_depth',\n",
        "    'document_hover_count_raw',\n",
        "    'document_hover_count',\n",
        "    'document_hover_depth',\n",
        "    'ad_hover_count',\n",
        "    'ad_hover_count_top',\n",
        "    'ad_hover_count_side',\n",
        "    'ad_hover_count_bot',\n",
        "    'ad_click_count',\n",
        "    'depth',\n",
        "    'time_per_snippet',\n",
        "    'time_per_document',\n",
        "    'query_length',\n",
        "    'query_tokens_count'\n",
        "]\n",
        "\n",
        "seed = 7\n",
        "num_splits = 5\n",
        "max_num_attributes = 5\n",
        "feature_selector = SelectKBest(mutual_info_classif, k=max_num_attributes)\n",
        "\n",
        "feature_scores = []\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(seed)\n",
        "set_random_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66hPaFs37I3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data preparation\n",
        "def dataset_preparation(dataset_df, num_classes, feature_names, apply_feature_selection=False):\n",
        "    label_encoder = preprocessing.LabelEncoder()\n",
        "    \n",
        "    # split into input (X) and output (Y) variables\n",
        "    X = preprocessing.scale(dataset_df[feature_names].to_numpy())\n",
        "    y = label_encoder.fit_transform(dataset_df.iloc[:, -1].to_numpy().tolist())\n",
        "\n",
        "    if apply_feature_selection:\n",
        "        X = feature_selector.fit_transform(X, y)\n",
        "        num_features = X.shape[-1]\n",
        "        feature_scores.append(feature_selector.scores_)\n",
        "    else:\n",
        "        num_features = len(feature_names)\n",
        "\n",
        "    return Dataset(X=X, y=y, num_features=num_features, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8deesd_K3c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = {\n",
        "    \"FA_Binary\": dataset_preparation(FA_Binary, 2, attributes, apply_feature_selection=True),\n",
        "    \"NC_Binary\": dataset_preparation(NC_Binary, 2, attributes, apply_feature_selection=True),\n",
        "    \"Overall_Trio\": dataset_preparation(Overall_Trio, 3, attributes, apply_feature_selection=True)\n",
        "}\n",
        "\n",
        "for i, (dataset_name, dataset) in enumerate(datasets.items()):\n",
        "    print(\"Dataset name: {}\".format(dataset_name))\n",
        "    indices = np.argsort(feature_scores[i])[::-1][:10]\n",
        "    for ind in indices:\n",
        "        print(f\"- {attributes[ind]} ({feature_scores[i][ind]})\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkoYkOch7QOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_classification_model(input_size, output_size, hidden_sizes=[64], activation=\"relu\"):\n",
        "    \"\"\"\n",
        "        Creates a Feed-forward Neural Network (1-layer) with an output layer of 3 neurons.\n",
        "        Every output neuron represents a specific class (low, medium, high).\n",
        "\n",
        "        The final Softmax activation function is used to obtain a probability distribution \n",
        "        over the classes (low, medium, high)\n",
        "    \"\"\"\n",
        "    def model_definition():\n",
        "        model = Sequential()\n",
        "        model.add(Dense(hidden_sizes[0], input_dim=input_size, kernel_initializer='glorot_uniform', activation=activation))\n",
        "\n",
        "        for hidden_size in hidden_sizes[1:]:\n",
        "            model.add(Dense(hidden_size, kernel_initializer='glorot_uniform', activation=activation))\n",
        "            model.add(Dropout(0.35, seed=seed))\n",
        "\n",
        "        # output layer\n",
        "        model.add(Dense(output_size, kernel_initializer='glorot_uniform', activation=\"softmax\"))\n",
        "        adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(loss='sparse_categorical_crossentropy',  metrics=[\"accuracy\"], optimizer = adam)\n",
        "            \n",
        "        return model\n",
        "    \n",
        "    return model_definition\n",
        "\n",
        "def create_pipeline(input_size, output_size):\n",
        "    return {\n",
        "        \"neural\": KerasClassifier(\n",
        "            nn_classification_model(input_size, output_size, hidden_sizes=[32]),\n",
        "            epochs=50, \n",
        "            batch_size=32\n",
        "        ),\n",
        "        \"neural_32_16\": KerasClassifier(\n",
        "            nn_classification_model(input_size, output_size, hidden_sizes=[32, 16]),\n",
        "            epochs=50, \n",
        "            batch_size=32\n",
        "        ),\n",
        "        \"neural_32_16_8\": KerasClassifier(\n",
        "            nn_classification_model(input_size, output_size, hidden_sizes=[32, 16, 8]),\n",
        "            epochs=50, \n",
        "            batch_size=32\n",
        "        ),\n",
        "        \"svm\": svm.SVC(shrinking=False, decision_function_shape=\"ovo\", random_state=seed),\n",
        "        \"decision_tree\": DecisionTreeClassifier(criterion=\"entropy\", random_state=seed),\n",
        "        \"random_forest\": RandomForestClassifier(n_estimators=10, random_state=seed),\n",
        "        \"majority_class\": DummyClassifier(strategy=\"most_frequent\", random_state=seed),\n",
        "        \"logistic_regression\": LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", random_state=seed)\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8_qVDgQHaXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_results(filename, all_results):\n",
        "    model_names = list(list(all_results.values())[0].keys())\n",
        "\n",
        "    with open(filename, mode=\"w\") as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        \n",
        "        writer.writerow([\"Dataset\"] + model_names)\n",
        "        for dataset, results in all_results.items():\n",
        "            writer.writerow([dataset] + [results[model][\"mean_accuracy\"] for model in model_names])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9nQrPN45zVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = {\n",
        "    \"FA_Binary\": dataset_preparation(FA_Binary, 2, attributes),\n",
        "    \"NC_Binary\": dataset_preparation(NC_Binary, 2, attributes),\n",
        "    \"Overall_Trio\": dataset_preparation(Overall_Trio, 3, attributes)\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for dataset_name, dataset in datasets.items():\n",
        "    print(\"# Starting train/test procedure for dataset {}\".format(dataset_name))\n",
        "    all_results[dataset_name] = {}\n",
        "    pipeline = create_pipeline(dataset.num_features, dataset.num_classes)\n",
        "\n",
        "    for model_name, model in pipeline.items():\n",
        "        kfold = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n",
        "        scores = cross_val_score(model, dataset.X, dataset.y, cv=kfold)\n",
        "\n",
        "        all_results[dataset_name][model_name] = {\n",
        "            \"mean_accuracy\": scores.mean(),\n",
        "            \"std_accuracy\": scores.std()\n",
        "        }\n",
        "    \n",
        "    print(\"Model\\tAccuracy\")\n",
        "    for model, results in all_results[dataset_name].items():\n",
        "        print(f\"{model}\\t{results['mean_accuracy']}\")\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhm1E7gWKIBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_file = \"experiment_results_all_attributes.csv\"\n",
        "\n",
        "print(\"# Saving results to file {}\".format(results_file))\n",
        "save_results(results_file, all_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKzIKTKyKNLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = {\n",
        "    \"FA_Binary\": dataset_preparation(FA_Binary, 2, attributes, apply_feature_selection=True),\n",
        "    \"NC_Binary\": dataset_preparation(NC_Binary, 2, attributes, apply_feature_selection=True),\n",
        "    \"Overall_Trio\": dataset_preparation(Overall_Trio, 3, attributes, apply_feature_selection=True)\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for dataset_name, dataset in datasets.items():\n",
        "    print(\"# Starting train/test procedure for dataset {}\".format(dataset_name))\n",
        "    all_results[dataset_name] = {}\n",
        "    pipeline = create_pipeline(dataset.num_features, dataset.num_classes)\n",
        "\n",
        "    for model_name, model in pipeline.items():\n",
        "        kfold = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n",
        "        scores = cross_val_score(model, dataset.X, dataset.y, cv=kfold)\n",
        "\n",
        "        all_results[dataset_name][model_name] = {\n",
        "            \"mean_accuracy\": scores.mean(),\n",
        "            \"std_accuracy\": scores.std()\n",
        "        }\n",
        "    \n",
        "    print(\"Model\\tAccuracy\")\n",
        "    for model, results in all_results[dataset_name].items():\n",
        "        print(f\"{model}\\t{results['mean_accuracy']}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1hU1RCG6CD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_file = \"experiment_results_with_feature_selection.csv\"\n",
        "\n",
        "print(\"# Saving results to file {}\".format(results_file))\n",
        "save_results(results_file, all_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTWBB2PIb82u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}